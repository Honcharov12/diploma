\section{Аналіз моделі ARIMA}
%\section{АНАЛІЗ МОДЕЛІ ARIMA}
\label{sec:ARIMA}

Метод авторегресії проінтегрованого ковзного середнього є досить потужним і гнучким, але він також є складним. Для кращого розуміння, необхідно розглянути певні моделі, алгоритми та поняття.

\subsection{Автокореляційна функція}

Автоковаріацією $\gamma_{k}$ часового ряду $x_{t}$ з затримкою $k$ називають вираз, наведений у формулі \ref{eq:autocov}.

\begin{equation}\label{eq:autocov}
\gamma_{k} = cov[x_{t}, x_{t+k}] = M[(x_{t} - a) \cdot (x_{t+k} - a)],
\end{equation}

\noindent де $a$ $-$ математичне очікування у перерізі ряду.

Також треба зазначити, що дисперсія часового ряду находиться за формулою \ref{eq:cov}.

\begin{equation}\label{eq:cov}
\sigma^{2}_{x} = M[(x_{t} - a)^2] = \gamma_{0}.
\end{equation}

Для отримання статистичної оцінки $\gamma^{*}_{k}$ автоковаріації $\gamma_{k}$ використовують вираз \ref{eq:stats}.

\begin{equation}\label{eq:stats}
\hat{\gamma}_{k} = \frac{1}{N-k}\sum^{N-k}_{t=1}(x_{t} - \hat{a}) \cdot (x_{t+k} - \hat{a}).
\end{equation}

Автоковаріація $\gamma_{k}$ характеризує ступінь лінійного зв'язку між значеннями часового ряду $x_{t}$ та $x_{t+k}$[4].

Автокореляційна функція(АКФ) $-$ це характеристика сигналу, яка допомагає знаходити повторювані відрізки сигналу або визначати несучу частоту сигналу, прихованих через накладення шуму та коливань на інших частотах[5].

Вона має наступний вигляд(формула \ref{eq:akf}):

\begin{equation}\label{eq:akf}
\rho(k) = \frac{\gamma_{k}}{\gamma_{0}} = \frac{M[(x_{t} - a) \cdot (x_{t+k} - a)]}{M[(x_{t} - a)^{2}]}.
\end{equation}

Статистичну оцінку АКФ можна отримати з формули \ref{eq:akfstats}.

\begin{equation}\label{eq:akfstats}
\hat{\rho}(k) = \frac{\frac{1}{N-k}\sum^{N-k}_{t=1}(x_{t} - \hat{a}) \cdot (x_{t+k} - \hat{a})}{\frac{1}{N}\sum^{N}{t=1}(x_{t} - \hat{a})^{2}},
\end{equation}

\noindent де $\hat{a} = \frac{1}{N}\sum^{N}_{t=1}x_{t}$.

\subsection{Часткова автокореляційна функція}

Часткова автокореляція вимірює кореляцію між рівнями часового ряду $x_{t}$ та $x_{t+k}$, не враховуючи вплив проміжних рівнів ряду. Такий показник кореляції між елементами ряду є більш інформативним[2].

Часткова автокореляція $k$-го порядку знаходиться як величина $\varphi_{kk}$, яку можна отримати з рівнянь Юла-Уокера (формула \ref{eq:pacf}).

\begin{equation}\label{eq:pacf}
\left[
\begin{array}{ccccc}
1 & \rho(1) & \rho(2) & \dots & \rho(k-1) \\
\rho(1) & 1 & \rho(1) & \dots & \rho(k-2) \\
\cdots & \vdots & \vdots & \ddots & \vdots \\
\rho(k-1) & \rho(k-2) & \rho(k-3) & \dots & 1
\end{array}
\right] \cdot
\left[
\begin{array}{c}
\varphi_{k1} \\
\varphi_{k2} \\
\vdots \\
\varphi_{kk}
\end{array}
\right]
=
\left[
\begin{array}{c}
\rho(1) \\
\rho(2) \\
\vdots \\
\rho(k)
\end{array}
\right]
\end{equation}

Знайдемо рішення для $k = 1, 2, 3$:

\[
\varphi_{11} = \rho(1), \varphi_{22} = \frac{\left|\begin{array}{cc} 1 & \rho(1) \\ \rho(1) & \rho(2) \end{array} \right|}{\left|\begin{array}{cc} 1 & \rho(1) \\ \rho(1) & 1 \end{array}\right|},
\varphi_{33} = \frac{\left|\begin{array}{ccc} 1 & \rho(1) & \rho(2) \\ \rho(1) & 1 & \rho(2) \\ \rho(2) & \rho(1) & \rho(3) \end{array}\right|}{\left|\begin{array}{ccc} 1 & \rho(1) & \rho(2) \\ \rho(1) & 1 & \rho(1) \\ \rho(2) & \rho(1) & 1 \end{array}\right|}.
\]

\subsection{Процеси авторегресії (AR)}

Авторегресійні моделі широко використовуються для опису стаціонарних випадкових процесів. Характерною особливістю стаціонарних часових рядів є те, що їх імовірнісні властивості рядів не змінюються в часі. Інакше кажучи, функції розподілу стаціонарних динамічних рядів не змінюються при зсуві часу[6].

В моделі, яка має вид формули \ref{eq:autoregression}, авторегресії поточне значення процесу $x_{t}$ подається у вигляді лінійної комбінації кінцевого числа попередніх значень процесу та білого шуму $e_{t}$:

\begin{equation}\label{eq:autoregression}
x_{t} = \alpha_{1}x_{t-1} + \alpha_{2}x_{t-2} + \dots + \alpha_{p}x_{t-p} + e_{t},
\end{equation}

\noindent при цьому припускається, що поточне значення $e_{t}$ не корелює з лагами $x_{t}$. Така модель називаеється авторегресією $p$-го порядку і позначається $AR(p)$ (від англійського autoregression).

Використовуючи лаговий оператор $L$, можемо отримати наступне рівняння авторегресії[2]:

$$
(1 - \alpha_{1}L - \alpha_{2}L^{2} - \dots - \alpha_{p}L^{p})x_{t} = e_{t},
$$

\noindent або завдяки лаговому поліному $\alpha(L) = 1 - \alpha_{1}L - \alpha_{2}L^{2} - \dots - \alpha_{p}L^{p}$ отримуємо більш компактну формулу \ref{eq:arL}.

\begin{equation}\label{eq:arL}
\alpha(L)x_{t} = e_{t}.
\end{equation}

\subsubsection{Оцінювання авторегресій}

Загалом,  автокореляційна функція стаціонарного процесу авторегресії є комбінацією згасаючих експонент та згасаючих синусоїд.

Теоретично вибіркова автокореляційна функція може бути інтсрументом для розпізнавання авторегресійного процесу. На практиці, для невеликих рядів автокореляційна функція не єє досить ефективною. Проте вивчення автокореляційної функції є хорошим початком у вивчення системи.

Термін "авторегресія" для позначення моделі (\ref{eq:autoregression}) використовується тому що вона фактично є моделлю регресії, в котрій регресорами служать лаги вивчаємого рялу $x_{t}$. По визначеню авторегресії помилки $e_{t}$ є білим шумом та некорельовані з лагами $x_{t}$. Таким чином, виконані всі основні припущення регресійного аналізу: помилки мають нульове математичне очікування, некорельовані з регресорами, не автокорельовані та гомоскедастичні. Таким чином, модель (\ref{eq:autoregression}) можна оцінювати за допомогою методу найменьших квадратів.

Часткова автокореляційна функція може виявитись корисною в вирішені задачі ідентифікації моделі часового ряду: якщо вона швидко згасає, то це авторегресія, та її порядок треба обрати у відповідності з останнім великим значенням часткової автокореляційної функції.

Після визначення порядку моделі $p$ коефіціенти моделі $\alpha_{j}$ знаходяться з рішення системи рівнянь:

\[
	\left\{
	\begin{aligned}
	\hat{\rho}(1) &= \hat{\alpha}_{1} + \hat{\alpha}_{2}\hat{\rho}(1) + \dots + \hat{\alpha}_{p}\hat{\rho}(p-1) \\
	\hat{\rho}(2) &= \hat{\alpha}_{1}\hat{\rho}(1) + \hat{\alpha}_{2} + \dots + \hat{\alpha}_{p}\hat{\rho}(p-2) \\
	&\;\;\vdots \notag \\
	\hat{\rho}(p) &= \hat{\alpha}_{1}\hat{\rho}(p-1) + \hat{\alpha}_{2}\hat{\rho}(p-2) + \dots + \hat{\alpha}_{p} \\
	\end{aligned}
	\right.
\]

\noindent де $\hat{\rho}(j)$ $-$ оцінені значення автокореляційної функції.

\subsection{Процеси ковзного середнього (MA)}

Іншою широко розповсюдженою моделлю в аналізі часових рядів є модель ковзного середнього, коли $x_{t}$ лінійно залежить від кінцевого числа попередніх значень $e$. Модель наведено у формулі \ref{eq:movingaverage}.

\begin{equation}\label{eq:movingaverage}
x_{t} = e_{t} - \theta_{1}e_{t-1} - \theta_{2}e_{t-2} - \dots - \theta_{q}e_{t-q}.
\end{equation}

Модель ковзного середнього $q$-го порядку позначають $MA(q)$ (від англійського moving average). Дану модель можна записати в більш стислій формулі \ref{eq:maL}.

\begin{equation}\label{eq:maL}
x_{t} = \theta(L)e_{t}
\end{equation}

\noindent при $\theta(L) = 1 - \theta_{1}L - \theta_{2}L^{2} - \dots - \theta_{q}L^{q}$.

\subsubsection{Оцінювання ковзних середніх}

Для оцінки параметрів MA-моделей використання звичайного методу найменьших квадратів не є доцільним, оскільки суму квадратів залишків не можна виразити аналітично через значення ряду. Можна використовувати метод максимальної правдоподібності за умови припущення нормального розподілу. Коваріаційна матриця необхідна для оцінки отримується зі стандартних формул коваріації за умови нормального розподілення $-$ математичне очікування дорівнює нулю. Потім використовують чисельні методи максимізації логарифмічної функції правдободібності.

Якщо всю значення часткової автокореляційної функції послідовно міняють знак, та починаючи з порядку $k$ дорівнюють нулю (статистично незначно відрізняються від нуля), то часовий ряд можна описати моделлю ковзного середнього порядка $q=r-1$.

\subsection{Процеси авторегресії-ковзного середнього (ARMA)}

В моделі часового ряду $x_{t}$ на основі авторегресії порядку $p$ у формуванні поточного значення ряду бере участь лише один поточний імпульс білого шуму $e_{t}$. Природно припустити, що якщо від цього імпульсу at відняти зважену суму $q$ попередніх значень білого шуму, то отримаємо модель, яка буде більш адекватно віддзеркалювати його властивості, оскільки крім авторегресії ця модель буде враховувати ще й ковзне середнє процесу[4].

Така модель часового ряду $x_{t}$ носить назву модель авторегресії — ковзного середнього $ARMA(p,q)$ (від англійського autoregressive moving-average). Модель $ARMA$ має вигляд формули \ref{eq:arma}.

\begin{equation}\label{eq:arma}
x_{t} = \alpha_{1}x_{t-1} + \dots + \alpha_{p}x_{t-p} + e_{t} - \theta_{1}e_{t-1} - \dots - \theta_{q}e_{t-q},
\end{equation}

\noindent або, з використанням оператору лагу:

\[
(1 - \alpha_{1}L - \alpha_{2}L^{2} - \dots - \alpha_{p}L^{p})x_{t} = (1 - \theta_{1}L - \theta_{2}L^{2} - \dots - \theta_{q}L^{q})e_{t}.
\]

В операторній формі модель виглядає так, як наведено у формулі \ref{eq:armaL}.

\begin{equation}\label{eq:armaL}
\alpha(L)x_{t} = \theta(L)e_{t},
\end{equation}

\noindent де $\alpha(L)$ $-$ оператор авторегресії, $\theta(L)$ $-$ оператор ковзного середнього.

Модель (\ref{eq:arma}) отримала назву моделі Бокса-Дженкінса. Така модель може інтерпретуватися як лінійна модель множинної регресії, в якій в якості пояснюють змінних виступають минулі значення самої залежною змінною, а в якості регресійного залишку - ковзаючі середні з елементів білого шуму.

Якщо ми припускаємо, що деякий спостережуваний часовий ряд $x_{t}$ породжується моделлю ARMA, то при цьому виникає проблема підбору конкретної моделі з цього класу, вирішення якої передбачає три етапи:

\begin{enumerate}
	\item ідентифікація моделі;
	\item оцінювання моделі;
	\item діагностика.
\end{enumerate}

На етапі ідентифікації проводиться вибір деякої конкретної моделі з усього класу ARMA, обираються значення $p$ і $q$. Використовувані при цьому процедури є не цілком точними, що може при подальшому аналізі привести до висновку про непридатність ідентифікованої моделі і необхідності заміни її альтернативною моделлю. 

На цьому ж етапі робляться попередні грубі оцінки коефіцієнтів ідентифікованої моделі.

На другому етапі проводиться уточнення оцінок коефіцієнтів моделі з використанням ефективних статистичних методів. Для оцінених коефіцієнтів обчислюються наближені стандартні помилки, що дають можливість, при додаткових припущеннях про розподіли випадкових величин $X_{1}, X_{2}, \dots,$ будувати довірчі інтервали для цих коефіцієнтів і перевіряти гіпотези про їх реальні значеннях з метою уточнення специфікації моделі.

На третьому етапі застосовуються різні діагностичні процедури перевірки адекватності обраної моделі наявними даними. Неадекватності, виявлені в процесі такої перевірки, можуть вказати на необхідне коригування моделі, після чого проводиться новий цикл підбору, до тих пір, поки не буде отримана задовільна модель. 

Зрозуміло, якщо ми маємо справу з ситуацією, коли вже є досить відпрацьована і розумно интерпретируемая модель еволюції того чи іншого показника, можна обійтися і без етапу ідентифікації[2].

\subsubsection{Ідентифікація стаціонарної моделі ARMA}

Взагалі кажучи, ідентифікація - це досить груба процедура (послідовність процедур), метою якої є визначення деякої області прийнятних значень характеристик порядку $p$ і $q$ моделі $ARMA(p, q)$, яка в ході подальших досліджень повинна бути зведена до конкретних їх величинам.

Зазвичай в цій частині ідентифікація супроводжується процедурами оцінки параметрів альтернативних варіантів моделей і вибору найкращого з них на основі використання критеріїв якості.

Загальна ідея ідентифікації моделі ARMA (p, q) полягає в тому, що властивості реального процесу і властивості найкращої моделі повинні бути близькі один до одного[7].

Для визначення порядку моделі може застосовуватися дослідження таких характеристик часового ряду, як його автокореляційна функція і приватна автокореляційна функція. Для визначення коефіцієнтів застосовуються такі методи, як метод найменших квадратів і метод максимальної правдоподібності.

Оскільки вибіркові коефіцієнти автокореляції можуть характеризуватися досить великими помилками і, крім того, сильними кореляційними взаємозв'язками між собою, то на практиці точного подібності між «теоретичної» і «емпіричної» автокореляційної функції очікувати не слід, особливо при великих зрушеннях. 

Наприклад, внаслідок статистичного взаємозв'язку між коефіцієнтами автокореляції процесу щодо значущі рівні вибіркових коефіцієнтів автокореляції (сплески) можуть мати місце і в областях зрушень, де їх теоретичні аналоги близькі до нуля. Тому при зіставленні теоретичних і вибіркових автокореляційних функцій зазвичай враховують лише їх основні характеристики. Саме їх збіг дозволяє значно звузити коло прийнятних для опису реального процесу варіантів моделі. Остаточний вибір на користь однієї з них зазвичай робиться за результатами етапів оцінювання та діагностики моделей.

\subsubsection{Висновки}

Модель $ARMA$ поєднує в собі властивості як авторегресії, так і ковзної середньої. А в зв'язку з тим, що на ділянці прогнозу помилки звертаються в нуль (елементи МА звертаються в нуль), всі прогнозовані траєкторії $ARMA(p, q)$ будуть відповідати траєкторіях AR (p). Однак це не означає, що змінна середня не потрібна - її облік дозволяє більш точно апроксимувати ряд даних і відсікти непотрібні елементи авторегресії, які виникли б через зв'язок між AR і МА[8].

\subsection{Модель авторегресії—проінтегрованого ковзного середнього}

Всі моделі часових рядів, що побудовані у попередніх підрозділах, базувались на умові стаціонарності цих рядів. Але у повсякденному житті постійно стикаємося і з нестаціонарними випадковими процесами. Наприклад, процес пуску потужного електропривода, який працює на навантаження випадкового характеру (екскавація, конвеєрна доставка кускової руди з шахти, перемелювання рудних матеріалів у дробарках та млинах тощо), має досить суттєві проміжки нестаціонарності[9].

Авторегресійне-проінтегроване ковзне середнє (від англійського $autoregressive \, integrated \, moving \, average$, $ARIMA$) є узагальненням моделі авторегресійного змінного середнього. Ці моделі використовуються при роботі з тимчасовими рядами для більш глибокого розуміння даних або передбачення майбутніх точок ряду. Зазвичай модель згадується, як $ARIMA(p,d,q)$, де $p$, $d$ і $q$ - цілі невід'ємні числа, що характеризують порядок для частин моделі (відповідно авторегресивної, інтегрованої і ковзного середнього)[10].

Як модифікація $ARMA(p,q)$-процесу, $ARIMA(p,d,q)$-процес $–$ це $d$-кратне використання оператора скінченних різниць $\Delta=1-L$ до початкового часового ряду $x_{t}$[11]. Його описують рівнянням \ref{eq:ARIMA}.

\begin{equation}\label{eq:ARIMA}
\alpha(L)\Delta^{d}x_{t} = \theta(L)e_{t}
\end{equation}

\noindent де $d$ $-$ це порядок різниці.

Побудова $ARIMA(p,d,q)$ моделі часового ряду складається з таких етапів:

\begin{enumerate}
	\item визначення загального класу моделей;
	\item вибір моделі (тобто, значень $p$, $d$, $q$) для експериментальної перевірки;
	\item оцінка параметрів під час експериментальної перевірки моделі (тобто, обчислення параметрів $\alpha_{1}, \alpha_{2}, \dots, \alpha_{p}, \theta_{1}, \theta_{2}, \dots, \theta_{q}$);
	\item діагностика моделі (перевірка того, чи не має досліджуваний часовий ряд властивостей, що суперечать одержаній моделі);
	\item використання моделі для виконання прогнозу. 
\end{enumerate}

У такому підході Бокса–Дженкінса не передбачено конкретної моделі для прогнозування досліджуваного часового ряду. Задається лише загальний клас моделей, що описують часовий ряд і дають змогу у деякий спосіб виражати поточне значення параметра ряду через його попередні значення. Алгоритм сам обере найбільш оптимальну модель для прогнозу. Для його реалізації використовують ітераційний підхід. У виборі моделі враховують як якісні характеристики, так і кількість її параметрів[11]. 

\subsubsection{Ідентифікація моделі ARIMA}

На етапі ідентифікації моделі необхідно виконати перевірку часового ряду на стаціонарність. Для цього найчастіше використовується візуальний аналіз вибіркової автокореляційної і часткової автокореляційної функцій. Для стаціонарних часових рядів автокореляціна і часткова автокореляційна функції швидко спадають після декількох перших значень. Якщо ж графіки спадають повільно, то часовий ряд може виявитися нестаціонарним. 

Нестаціонарні часові ряди можна перетворити в стаціонарні шляхом взяття різниць. Вихідний ряд замінюється рядом різниць. Взяття різниць може повторюватися декілька раз. Число повторень взяття різниць, необхідних для отримання стаціонарної поведінки даних, позначається параметром $d$. Також на цьому етапі використовуються статистичні тести на наявність одиничного кореня.

Після отримання стаціонарного ряду досліджується характер поведінки вибіркових автокореляційної і часткової автокореляційної функцій і висуваються гіпотези про значення параметрів $p$ і $q$. Під час цього формується базовий набір $ARIMA$–моделей.

На другому етапі виконується оцінка параметрів цих моделей. Для цих цілей найчастіше використовується метод максимальної правдоподібності. Для отримання початкових значень параметрів ARIMA-моделі використовують рівняння Юла–Уокера, а для уточнення оцінок параметрів – метод Марквардта. Для кожної з обраних моделей оцінюють її параметри та обчислюють залишки[11]. 

Для перевірки кожної з отриманих моделей на адекватність аналізується її ряд залишків. 

У декватної моделі ряд залишків повинен бути подібним на "білий" шум. Також для перевірки гіпотези про те, що спостережувані дані є реалізацією "білого" шуму, використовується $Q$–статистика. $Q$–статистика Льюнга–Бокса визначається так, як наведено у формулі \ref{eq:Qstats}.

\begin{equation}\label{eq:Qstats}
Q = N(N+2)\sum_{k=1}^{M}\frac{\rho_{k}^{2}}{N-k}
\end{equation}

\noindent де $N$ $-$ об’єм вибірки, $M$ $-$ кількість лагів, що тестуються $\rho_{k}$ $-$ коефіціенти автокореляційної функції. 

$Q$ має асимптотичний розподіл $\chi^{2}_{1-\alpha, M}$. Якщо $Q < \chi^{2}_{1-\alpha,M}$ $-$ то приймається гіпотеза про відсутність автокореляції до $M$-го порядку в досліджуваному ряді залишків.

Якщо в результаті перевірки декілька моделей є адекватними спостережуваним даним, то при кінцевому виборі враховуються фактори: підвищення точності; зменшення числа параметрів моделі. Ці вимоги об’єднані в критеріях Акаіке і Шварца, які побудовані на принципі штрафів за додаткові параметри моделі. Інформаційний критерій Акаіке, який наведено у формулі \ref{eq:AIC}.

\begin{equation}\label{eq:AIC}
AIC = ln(\hat{\sigma}^{2}) + \frac{2(p+q+1)}{N}
\end{equation}

\noindent де $\hat{\sigma}^{2}$ $-$ очікувана вибіркова дисперсія[11]. 

Байєсівський інформаційний критерій (критерій Шварца), який наведено у формулі \ref{eq:BIC}. 

\begin{equation}\label{eq:BIC}
BIC = ln(\hat{\sigma}^{2}) + \frac{(p+q+1)ln(N)}{N}
\end{equation}

Перший доданок в виразах (\ref{eq:AIC}) і (\ref{eq:BIC}) представляє собою штраф за велику дисперсію, а другий – штраф за використання додаткових змінних. Кращою серед декількох $ARIMA$–моделей вважається модель з меншою величиною $AIC$, $BIC$.

З допомогою отриманої моделі можна побудувати точний і інтервальний прогнози на $K$ кроків вперед. Для оцінки точності прогнозу використовуються стандартні критерії[11].

Середня абсолютна процентна похибка розраховується за формулою \ref{eq:MAPE}.

\begin{equation}\label{eq:MAPE}
MAPE = \frac{100}{K}\sum_{i=1}^{K}\left|\frac{x_{i} - \hat{x}_{i}}{x_{i}}\right|
\end{equation}

\noindent де $x_{i}$ $-$ спостережуване значення; $\hat{x}_{i}$ $-$ значення прогнозу; $K$ $-$ інтервал прогнозу. 

Якщо $MAPE < 10 \%$, то прогноз реалізований з високою точністю, при $10\%<MAPE<20\%$ $-$ прогноз добрий, при $20\%<MAPE<50\%$ $-$ прогноз задовільний, при $MAPE>50\%$ $-$ прогноз незадовільний.

Середня процентна похибка розраховується за формулою \ref{eq:MPE}.

\begin{equation}\label{eq:MPE}
MPE = \frac{100}{K}\sum_{i=1}^{K}\frac{x_{i} - \hat{x}_{i}}{x_{i}}
\end{equation}

\noindent яка дозволяє визначити зміщення отриманого прогнозу. 

Якщо отримана модель є незміщеною, то $MPE < 5\%$. Якщо в результаті розрахунків отримується велике від’ємне значення, то модель є з послідовним переоцінюванням. Якщо ж отримано велике додатне число, то модель - з послідовним недооцінюванням. 

Алгоритм Бокса–Дженкінса дозволяє виконувати достатньо точний короткочасний прогноз. Але необхідно відмітити, що не існує простого способу корекції параметрів ARIMA моделі для нових даних. Модель потрібно періодично повністю перебудовувати або вибирати зовсім нову модель[11]. 

\subsubsection{Переваги і недоліки моделей ARIMA}

Щоб підсумувати наш розгляд моделей ARIMA, обговоримо їх переваги та недоліки.

До очевидних переваг можна віднести те, що ці моделі мають дуже чітке математико-статистичне обгрунтування, що робить їх одними з найбільш науково обгрунтованих моделей з усієї безлічі моделей прогнозування тенденцій у часових рядах.

Ще однією перевагою є формалізована і найбільш докладно розроблена методика, слідуючи якій можна підібрати модель, найбільш підходящу до кожного конкретного часового ряду. Формальна процедура перевірки моделі на адекватність досить проста, а розроблені методики з автоматичного підбору найкращої ARIMA ще більше полегшують роботу.

Крім того, точкові і інтервальні прогнози випливають з самої моделі і не вимагають окремого оцінювання.

Один з явних недоліків моделей полягає у вимозі до рядів даних: для побудови адекватної моделі ARIMA потрібно не менше 40 спостережень, що на практиці не завжди можливо.

Другим серйозним недоліком є неадаптивность моделей авторегресії: при отриманні нових даних модель потрібно періодично переоцінювати.

Третій недолік полягає в тому, що побудова задовільною моделі ARIMA вимагає великих витрат ресурсів і часу[12].

\subsection{Висновки}

Моделі ARIMA досить гнучкі і можуть описувати широкий спектр характеристик часових рядів, що зустрічаються на практиці. Формальна процедура перевірки моделі на адекватність проста і доступна. Крім того, прогнози і інтервали передбачення слідують безпосередньо з підібраною моделі[7].

Для кожного конкретного випадку варто звертатися до своєї прогнозної моделі: будь то найпростіші моделі, моделі трендів, сезонної декомпозиції, моделі експоненціального згладжування або моделі авторегресії з ковзаючою середньою. Просто варто мати на увазі як позитивні, так і негативні сторони використовуваних моделей і спиратися на ті прогнози, щодо яких (на основі експертної думки та фундаментального аналізу галузі) можна сказати, що вони краще опишуть реальну ситуацію в майбутньому.
