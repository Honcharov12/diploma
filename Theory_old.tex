%\section{Огляд літературних джерел}
\section{ОГЛЯД ЛІТЕРАТУРНИХ ДЖЕРЕЛ}
\label{sec:Theory}

%\vspace{1.5em}

%\vskip 12pt

Усі методи виділення невипадкових складових моделі часового ряду можна поділити на два основних класи $-$ аналітичні та алгоритмічні. При використанні аналітичних методів припускають, що для опису динаміки часового ряду може бути використана аналітична функція, параметри якої можуть бути оцінені методами регресійного аналізу. Алгоритмічні методи дозволяють кількісно оцінити значення невипадкових компонент часового ряду у кожен момент часу [1].

Перш ніж перейти до певних аналітичних методів, необхідно розглянути фундаментальні поняття теорії аналізу часових рядів.

\subsection{Автокореляційна функція}

Автоковаріацією $\gamma_{k}$ часового ряду $x_{t}$ з затримкою $k$ називають вираз, наведений у формулі (\ref{eq:autocov}).

\begin{equation}\label{eq:autocov}
\gamma_{k} = cov[x_{t}, x_{t+k}] = M[(x_{t} - a) \cdot (x_{t+k} - a)],
\end{equation}

\noindent де $a$ $-$ математичне очікування у перерізі ряду.

\vspace{1.5em}

Також треба зазначити, що дисперсія часового ряду знаходиться наступним чином:

\begin{equation}\label{eq:cov}
\sigma^{2}_{x} = M[(x_{t} - a)^2] = \gamma_{0}.
\end{equation}

\vspace{1.5em}

Для отримання статистичної оцінки $\gamma^{*}_{k}$ автоковаріації $\gamma_{k}$ використовують вираз (\ref{eq:stats}).

\begin{equation}\label{eq:stats}
\hat{\gamma}_{k} = \frac{1}{N-k}\sum^{N-k}_{t=1}(x_{t} - \hat{a}) \cdot (x_{t+k} - \hat{a}).
\end{equation}

\vspace{1.5em}

Автоковаріація $\gamma_{k}$ характеризує ступінь лінійного зв'язку між значеннями часового ряду $x_{t}$ та $x_{t+k}$.

Автокореляційна функція (АКФ) $-$ це характеристика сигналу, яка допомагає знаходити повторювані відрізки сигналу або визначати несучу частоту сигналу, прихованих через накладення шуму та коливань на інших частотах [3].

Вона має наступний вигляд:

\begin{equation}\label{eq:akf}
\rho(k) = \frac{\gamma_{k}}{\gamma_{0}} = \frac{M[(x_{t} - a) \cdot (x_{t+k} - a)]}{M[(x_{t} - a)^{2}]}.
\end{equation}

\vspace{1.5em}

Статистичну оцінку АКФ можна отримати з формули (\ref{eq:akfstats}).

\begin{equation}\label{eq:akfstats}
\hat{\rho}(k) = \frac{\frac{1}{N-k}\sum^{N-k}_{t=1}(x_{t} - \hat{a}) \cdot (x_{t+k} - \hat{a})}{\frac{1}{N}\sum^{N}_{t=1}(x_{t} - \hat{a})^{2}},
\end{equation}

\noindent де $\hat{a} = \frac{1}{N}\sum^{N}_{t=1}x_{t}$.

\subsection{Часткова автокореляційна функція}

Часткова автокореляція вимірює кореляцію між рівнями часового ряду $x_{t}$ та $x_{t+k}$, не враховуючи вплив проміжних рівнів ряду. Такий показник кореляції між елементами ряду є більш інформативним [2].

Часткова автокореляція $k$-го порядку знаходиться як величина $\varphi_{kk}$, яку можна отримати з рівнянь Юла$-$Уокера:

\begin{equation}\label{eq:pacf}
\left[
\begin{array}{ccccc}
1 & \rho(1) & \rho(2) & \dots & \rho(k-1) \\
\rho(1) & 1 & \rho(1) & \dots & \rho(k-2) \\
\cdots & \vdots & \vdots & \ddots & \vdots \\
\rho(k-1) & \rho(k-2) & \rho(k-3) & \dots & 1
\end{array}
\right] \cdot
\left[
\begin{array}{c}
\varphi_{k1} \\
\varphi_{k2} \\
\vdots \\
\varphi_{kk}
\end{array}
\right]
=
\left[
\begin{array}{c}
\rho(1) \\
\rho(2) \\
\vdots \\
\rho(k)
\end{array}
\right].
\end{equation}

\vspace{1.5em}

Знайдемо рішення для $k = 1, 2, 3$:

\begin{equation}
\varphi_{11} = \rho(1), \varphi_{22} = \frac{\left|\begin{array}{cc} 1 & \rho(1) \\ \rho(1) & \rho(2) \end{array} \right|}{\left|\begin{array}{cc} 1 & \rho(1) \\ \rho(1) & 1 \end{array}\right|},
\varphi_{33} = \frac{\left|\begin{array}{ccc} 1 & \rho(1) & \rho(2) \\ \rho(1) & 1 & \rho(2) \\ \rho(2) & \rho(1) & \rho(3) \end{array}\right|}{\left|\begin{array}{ccc} 1 & \rho(1) & \rho(2) \\ \rho(1) & 1 & \rho(1) \\ \rho(2) & \rho(1) & 1 \end{array}\right|}.
\end{equation}

\subsection{Процеси авторегресії}

Авторегресійні моделі широко використовуються для опису стаціонарних випадкових процесів. Характерною особливістю стаціонарних часових рядів є те, що їх імовірнісні властивості рядів не змінюються в часі. Інакше кажучи, функції розподілу стаціонарних динамічних рядів не змінюються при зсуві часу [4].

В моделі (\ref{eq:autoregression}) поточне значення процесу $x_{t}$ подається у вигляді лінійної комбінації кінцевого числа попередніх значень процесу та білого шуму $e_{t}$:

\begin{equation}\label{eq:autoregression}
x_{t} = \alpha_{1}x_{t-1} + \alpha_{2}x_{t-2} + \dots + \alpha_{p}x_{t-p} + e_{t}.
\end{equation}

\vspace{1.5em}


При цьому припускається, що поточне значення $e_{t}$ не корелює з лагами $x_{t}$. Така модель називаеється авторегресією $p$-го порядку і позначається AR($p$) (від англійського autoregression).

Використовуючи лаговий оператор L, можемо отримати наступне рівняння авторегресії [2]:

\begin{equation}
(1 - \alpha_{1}L - \alpha_{2}L^{2} - \dots - \alpha_{p}L^{p})x_{t} = e_{t},
\end{equation}

\vspace{1.5em}

Або завдяки лаговому поліному:

\begin{equation}
\alpha(L) = 1 - \alpha_{1}L - \alpha_{2}L^{2} - \dots - \alpha_{p}L^{p}.
\end{equation}

\vspace{1.5em}

Отримуємо більш компактну форму:

\begin{equation}\label{eq:arL}
\alpha(L)x_{t} = e_{t}.
\end{equation}

Загалом,  автокореляційна функція стаціонарного процесу авторегресії є комбінацією згасаючих експонент та згасаючих синусоїд.

Теоретично вибіркова автокореляційна функція може бути інструментом для розпізнавання авторегресійного процесу. На практиці, для невеликих рядів автокореляційна функція не є досить ефективною. Проте вивчення автокореляційної функції є хорошим початком у вивчення системи.

Термін «авторегресія» для позначення моделі (\ref{eq:autoregression}) використовується тому що вона фактично є моделлю регресії, в котрій регресорами служать лаги вивчаємого ряду $x_{t}$. По визначеню авторегресії помилки $e_{t}$ є білим шумом та некорельовані з лагами $x_{t}$. Таким чином, виконані всі основні припущення регресійного аналізу: помилки мають нульове математичне очікування, некорельовані з регресорами, не автокорельовані та гомоскедастичні. Таким чином, модель (\ref{eq:autoregression}) можна оцінювати за допомогою методу найменьших квадратів.

Часткова автокореляційна функція може виявитись корисною в вирішені задачі ідентифікації моделі часового ряду: якщо вона швидко згасає, то це авторегресія, та її порядок треба обрати у відповідності з останнім великим значенням часткової автокореляційної функції.

Після визначення порядку моделі $p$ коефіціенти моделі $\alpha_{j}$ знаходяться з рішення системи рівнянь:

\begin{equation}
	\left\{
	\begin{aligned}
	\hat{\rho}(1) &= \hat{\alpha}_{1} + \hat{\alpha}_{2}\hat{\rho}(1) + \dots + \hat{\alpha}_{p}\hat{\rho}(p-1) \\
	\hat{\rho}(2) &= \hat{\alpha}_{1}\hat{\rho}(1) + \hat{\alpha}_{2} + \dots + \hat{\alpha}_{p}\hat{\rho}(p-2) \\
	&\;\;\vdots \notag \\
	\hat{\rho}(p) &= \hat{\alpha}_{1}\hat{\rho}(p-1) + \hat{\alpha}_{2}\hat{\rho}(p-2) + \dots + \hat{\alpha}_{p} \\
	\end{aligned}
	\right.,
\end{equation}

\noindent де $\hat{\rho}(j)$ $-$ оцінені значення автокореляційної функції.

\vspace{1.5em}

Модель авторегресії $-$ досить потужний метод аналізу, але вона потребує стаціонарності часового ряду $-$ це її головний мінус, який не дозволяє використовувати її у даній роботі.

\subsection{Процеси ковзного середнього}

Іншою широко розповсюдженою моделлю в аналізі часових рядів є модель ковзного середнього, коли $x_{t}$ лінійно залежить від кінцевого числа попередніх значень $e$. Модель наведено у формулі (\ref{eq:movingaverage}).

\begin{equation}\label{eq:movingaverage}
x_{t} = e_{t} - \theta_{1}e_{t-1} - \theta_{2}e_{t-2} - \dots - \theta_{q}e_{t-q}.
\end{equation}

\vspace{1.5em}


Модель ковзного середнього $q$-го порядку позначають $MA(q)$ (від англійського moving average). Дану модель можна записати в більш стислій формулі (\ref{eq:maL}).

\begin{equation}\label{eq:maL}
x_{t} = \theta(L)e_{t},
\end{equation}

\noindent при $\theta(L) = 1 - \theta_{1}L - \theta_{2}L^{2} - \dots - \theta_{q}L^{q}$.

\vspace{1.5em}


Для оцінки параметрів MA-моделей використання звичайного методу найменьших квадратів не є доцільним, оскільки суму квадратів залишків не можна виразити аналітично через значення ряду. Можна використовувати метод максимальної правдоподібності за умови припущення нормального розподілу. Коваріаційна матриця необхідна для оцінки отримується зі стандартних формул коваріації за умови нормального розподілення $-$ математичне очікування дорівнює нулю. Потім використовують чисельні методи максимізації логарифмічної функції правдободібності.

Якщо всю значення часткової автокореляційної функції послідовно міняють знак, та починаючи з порядку $k$ дорівнюють нулю (статистично незначно відрізняються від нуля), то часовий ряд можна описати моделлю ковзного середнього порядка $q=r-1$.

Модель ковзного середнього може працювати з нестаціонарними рядами, але вона ніяк не використовує інформацію про попередні значення часового ряду, ща не дозволяє використовувати її у даній роботі.

\subsection{Процеси авторегресії-ковзного середнього}

В моделі часового ряду $x_{t}$ на основі авторегресії порядку $p$ у формуванні поточного значення ряду бере участь лише один поточний імпульс білого шуму $e_{t}$. Природно припустити, що якщо від цього імпульсу at відняти зважену суму $q$ попередніх значень білого шуму, то отримаємо модель, яка буде більш адекватно віддзеркалювати його властивості, оскільки крім авторегресії ця модель буде враховувати ще й ковзне середнє процесу.

Така модель часового ряду $x_{t}$ носить назву модель авторегресії — ковзного середнього ARMA($p$,$q$) (від англійського autoregressive moving-average). Модель ARMA має наступний вигляд:

\begin{equation}\label{eq:arma}
x_{t} = \alpha_{1}x_{t-1} + \dots + \alpha_{p}x_{t-p} + e_{t} - \theta_{1}e_{t-1} - \dots - \theta_{q}e_{t-q}.
\end{equation}

\vspace{1.5em}

Або, з використанням оператору лагу:

\begin{equation}
(1 - \alpha_{1}L - \alpha_{2}L^{2} - \dots - \alpha_{p}L^{p})x_{t} = (1 - \theta_{1}L - \theta_{2}L^{2} - \dots - \theta_{q}L^{q})e_{t}.
\end{equation}

\vspace{1.5em}

В операторній формі модель виглядає так, як наведено у формулі (\ref{eq:armaL}).

\begin{equation}\label{eq:armaL}
\alpha(L)x_{t} = \theta(L)e_{t},
\end{equation}

\noindent де $\alpha(L)$ $-$ оператор авторегресії; \newline
\hspace*{15pt} $\theta(L)$ $-$ оператор ковзного середнього.

\vspace{1.5em}

Модель (\ref{eq:arma}) отримала назву моделі Бокса-Дженкінса. Така модель може інтерпретуватися як лінійна модель множинної регресії, в якій в якості пояснюють змінних виступають минулі значення самої залежною змінною, а в якості регресійного залишку - ковзаючі середні з елементів білого шуму.

Якщо ми припускаємо, що деякий спостережуваний часовий ряд $x_{t}$ породжується моделлю ARMA, то при цьому виникає проблема підбору конкретної моделі з цього класу, вирішення якої передбачає три етапи:

\begin{enumerate}
	\item ідентифікація моделі;
	\item оцінювання моделі;
	\item діагностика.
\end{enumerate}

На етапі ідентифікації проводиться вибір деякої конкретної моделі з усього класу ARMA, обираються значення $p$ і $q$. Використовувані при цьому процедури є не цілком точними, що може при подальшому аналізі привести до висновку про непридатність ідентифікованої моделі і необхідності заміни її альтернативною моделлю. 

На цьому ж етапі робляться попередні грубі оцінки коефіцієнтів ідентифікованої моделі.

На другому етапі проводиться уточнення оцінок коефіцієнтів моделі з використанням ефективних статистичних методів. Для оцінених коефіцієнтів обчислюються наближені стандартні помилки, що дають можливість, при додаткових припущеннях про розподіли випадкових величин $X_{1}, X_{2}, \dots, X_{N}$ будувати довірчі інтервали для цих коефіцієнтів і перевіряти гіпотези про їх реальні значеннях з метою уточнення специфікації моделі.

На третьому етапі застосовуються різні діагностичні процедури перевірки адекватності обраної моделі наявними даними. Неадекватності, виявлені в процесі такої перевірки, можуть вказати на необхідне коригування моделі, після чого проводиться новий цикл підбору, до тих пір, поки не буде отримана задовільна модель. 

Зрозуміло, якщо ми маємо справу з ситуацією, коли вже є досить відпрацьована і розумно интерпретируемая модель еволюції того чи іншого показника, можна обійтися і без етапу ідентифікації [2].

Взагалі кажучи, ідентифікація $-$ це досить груба процедура (послідовність процедур), метою якої є визначення деякої області прийнятних значень характеристик порядку $p$ і $q$ моделі ARMA($p$, $q$), яка в ході подальших досліджень повинна бути зведена до конкретних їх величинам.

Зазвичай в цій частині ідентифікація супроводжується процедурами оцінки параметрів альтернативних варіантів моделей і вибору найкращого з них на основі використання критеріїв якості.

Загальна ідея ідентифікації моделі ARMA($p$, $q$) полягає в тому, що властивості реального процесу і властивості найкращої моделі повинні бути близькі один до одного [5].

Для визначення порядку моделі може застосовуватися дослідження таких характеристик часового ряду, як його автокореляційна функція і приватна автокореляційна функція. Для визначення коефіцієнтів застосовуються такі методи, як метод найменших квадратів і метод максимальної правдоподібності.

Оскільки вибіркові коефіцієнти автокореляції можуть характеризуватися досить великими помилками і, крім того, сильними кореляційними взаємозв'язками між собою, то на практиці точного подібності між «теоретичної» і «емпіричної» автокореляційної функції очікувати не слід, особливо при великих зрушеннях. 

Наприклад, внаслідок статистичного взаємозв'язку між коефіцієнтами автокореляції процесу щодо значущі рівні вибіркових коефіцієнтів автокореляції (сплески) можуть мати місце і в областях зрушень, де їх теоретичні аналоги близькі до нуля. Тому при зіставленні теоретичних і вибіркових автокореляційних функцій зазвичай враховують лише їх основні характеристики. Саме їх збіг дозволяє значно звузити коло прийнятних для опису реального процесу варіантів моделі. Остаточний вибір на користь однієї з них зазвичай робиться за результатами етапів оцінювання та діагностики моделей.

Модель ARMA поєднує в собі властивості як авторегресії, так і ковзної середньої. А в зв'язку з тим, що на ділянці прогнозу помилки звертаються в нуль (елементи МА звертаються в нуль), всі прогнозовані траєкторії ARMA($p$, $q$) будуть відповідати траєкторіях AR($p$). Однак це не означає, що змінна середня не потрібна - її облік дозволяє більш точно апроксимувати ряд даних і відсікти непотрібні елементи авторегресії, які виникли б через зв'язок між AR і МА. Але модель потребує стаціонарності ряду, саме тому вона не підходить для вирішення задач даної роботи.

\subsection{Авторегресія — проінтегрованого ковзного середнього}

Всі моделі часових рядів, що побудовані у попередніх підрозділах, базувались на умові стаціонарності цих рядів. Але у повсякденному житті постійно стикаємося і з нестаціонарними випадковими процесами. Наприклад, процес пуску потужного електропривода, який працює на навантаження випадкового характеру (екскавація, конвеєрна доставка кускової руди з шахти, перемелювання рудних матеріалів у дробарках та млинах тощо), має досить суттєві проміжки нестаціонарності.

Авторегресійне-проінтегроване ковзне середнє (від англійського autoregressive integrated moving average, ARIMA) є узагальненням моделі авторегресійного змінного середнього. Ці моделі використовуються при роботі з тимчасовими рядами для більш глибокого розуміння даних або передбачення майбутніх точок ряду. Зазвичай модель згадується, як ARIMA($p$,$d$,$q$), де $p$, $d$ і $q$ - цілі невід'ємні числа, що характеризують порядок для частин моделі (відповідно авторегресивної, інтегрованої і ковзного середнього).

Як модифікація ARMA($p$,$q$)-процесу, ARIMA($p$,$d$,$q$)-процес $-$ це $d$-кратне використання оператора скінченних різниць $\Delta=1-L$ до початкового часового ряду $x_{t}$ [6]. Його описують рівнянням \ref{eq:ARIMA}.

\begin{equation}\label{eq:ARIMA}
\alpha(L)\Delta^{d}x_{t} = \theta(L)e_{t}
\end{equation}

\noindent де $d$ $-$ це порядок різниці.

\vspace{1.5em}

Побудова ARIMA($p$,$d$,$q$) моделі часового ряду складається з таких етапів:

\begin{enumerate}
	\item визначення загального класу моделей;
	\item вибір моделі (тобто, значень $p$, $d$, $q$) для експериментальної\newline \hspace*{-18mm}перевірки;
	\item оцінка параметрів під час експериментальної перевірки моделі\newline 
\hspace*{-18mm}(тобто, обчислення параметрів $\alpha_{1}, \alpha_{2}, \dots, \alpha_{p}, \theta_{1}, \theta_{2}, \dots, \theta_{q}$);
	\item діагностика моделі (перевірка того, чи не має досліджуваний\newline \hspace*{-18mm}часовий ряд властивостей, що суперечать одержаній моделі);
	\item використання моделі для виконання прогнозу. 
\end{enumerate}

У такому підході Бокса–Дженкінса не передбачено конкретної моделі для прогнозування досліджуваного часового ряду. Задається лише загальний клас моделей, що описують часовий ряд і дають змогу у деякий спосіб виражати поточне значення параметра ряду через його попередні значення. Алгоритм сам обере найбільш оптимальну модель для прогнозу. Для його реалізації використовують ітераційний підхід. У виборі моделі враховують як якісні характеристики, так і кількість її параметрів [6]. 

\subsubsection{Ідентифікація моделі ARIMA}

На етапі ідентифікації моделі необхідно виконати перевірку часового ряду на стаціонарність. Для цього найчастіше використовується візуальний аналіз вибіркової автокореляційної і часткової автокореляційної функцій. Для стаціонарних часових рядів автокореляціна і часткова автокореляційна функції швидко спадають після декількох перших значень. Якщо ж графіки спадають повільно, то часовий ряд може виявитися нестаціонарним. 

Нестаціонарні часові ряди можна перетворити в стаціонарні шляхом взяття різниць. Вихідний ряд замінюється рядом різниць. Взяття різниць може повторюватися декілька раз. Число повторень взяття різниць, необхідних для отримання стаціонарної поведінки даних, позначається параметром $d$. Також на цьому етапі використовуються статистичні тести на наявність одиничного кореня.

Після отримання стаціонарного ряду досліджується характер поведінки вибіркових автокореляційної і часткової автокореляційної функцій і висуваються гіпотези про значення параметрів $p$ і $q$. Під час цього формується базовий набір ARIMA–моделей.

На другому етапі виконується оцінка параметрів цих моделей. Для цих цілей найчастіше використовується метод максимальної правдоподібності. Для отримання початкових значень параметрів ARIMA-моделі використовують рівняння Юла–Уокера, а для уточнення оцінок параметрів – метод Марквардта. Для кожної з обраних моделей оцінюють її параметри та обчислюють залишки [6]. 

Для перевірки кожної з отриманих моделей на адекватність аналізується її ряд залишків. 

У декватної моделі ряд залишків повинен бути подібним на «білий» шум. Також для перевірки гіпотези про те, що спостережувані дані є реалізацією «білого» шуму, використовується $Q$–статистика. $Q$–статистика Льюнга–Бокса визначається так, як наведено у формулі \ref{eq:Qstats}.

\begin{equation}\label{eq:Qstats}
Q = N(N+2)\sum_{k=1}^{M}\frac{\rho_{k}^{2}}{N-k},
\end{equation}

\noindent де $N$ $-$ об’єм вибірки; \newline
\hspace*{15pt}$M$ $-$ кількість лагів, що тестуються; \newline
\hspace*{15pt}$\rho_{k}$ $-$ коефіціенти автокореляційної функції. 

\vspace{1.5em}

$Q$ має асимптотичний розподіл $\chi^{2}_{1-\alpha, M}$. Якщо $Q < \chi^{2}_{1-\alpha,M}$ $-$ то приймається гіпотеза про відсутність автокореляції до $M$-го порядку в досліджуваному ряді залишків.

Якщо в результаті перевірки декілька моделей є адекватними спостережуваним даним, то при кінцевому виборі враховуються фактори: підвищення точності; зменшення числа параметрів моделі. Ці вимоги об’єднані в критеріях Акаіке і Шварца, які побудовані на принципі штрафів за додаткові параметри моделі. Інформаційний критерій Акаіке, який наведено у формулі \ref{eq:AIC}.

\begin{equation}\label{eq:AIC}
AIC = ln(\hat{\sigma}^{2}) + \frac{2(p+q+1)}{N},
\end{equation}

\noindent де $\hat{\sigma}^{2}$ $-$ очікувана вибіркова дисперсія [6]. 

\vspace{1.5em}

Байєсівський інформаційний критерій (критерій Шварца), який наведено у формулі \ref{eq:BIC}. 

\begin{equation}\label{eq:BIC}
BIC = ln(\hat{\sigma}^{2}) + \frac{(p+q+1)ln(N)}{N}.
\end{equation}


\vspace{1.5em}

Перший доданок в виразах (\ref{eq:AIC}) і (\ref{eq:BIC}) представляє собою штраф за велику дисперсію, а другий – штраф за використання додаткових змінних. Кращою серед декількох ARIMA–моделей вважається модель з меншою величиною $AIC$, $BIC$.

З допомогою отриманої моделі можна побудувати точний і інтервальний прогнози на $K$ кроків вперед. Для оцінки точності прогнозу використовуються стандартні критерії [6].

Середня абсолютна процентна похибка розраховується за формулою:

\begin{equation}\label{eq:MAPE}
MAPE = \frac{100}{K}\sum_{i=1}^{K}\left|\frac{x_{i} - \hat{x}_{i}}{x_{i}}\right|,
\end{equation}

\noindent де $x_{i}$ $-$ спостережуване значення;\newline
\hspace*{15pt} $\hat{x}_{i}$ $-$ значення прогнозу;\newline
\hspace*{15pt} $K$ $-$ інтервал прогнозу. 

\vspace{1.5em}

Якщо $MAPE < 10 \%$, то прогноз реалізований з високою точністю, при $10\%<MAPE<20\%$ $-$ прогноз добрий, при $20\%<MAPE<50\%$ $-$ прогноз задовільний, при $MAPE>50\%$ $-$ прогноз незадовільний.

Середня процентна похибка розраховується за наступною формулою, яка дозволяє визначити зміщення отриманого прогнозу:

\begin{equation}\label{eq:MPE}
MPE = \frac{100}{K}\sum_{i=1}^{K}\frac{x_{i} - \hat{x}_{i}}{x_{i}}.
\end{equation}

\vspace{1.5em}

Якщо отримана модель є незміщеною, то $MPE < 5\%$. Якщо в результаті розрахунків отримується велике від’ємне значення, то модель є з послідовним переоцінюванням. Якщо ж отримано велике додатне число, то модель - з послідовним недооцінюванням. 

Алгоритм Бокса–Дженкінса дозволяє виконувати достатньо точний короткочасний прогноз. Але необхідно відмітити, що не існує простого способу корекції параметрів ARIMA моделі для нових даних. Модель потрібно періодично повністю перебудовувати або вибирати зовсім нову модель [6]. 
\newpage

\subsubsection{Переваги і недоліки моделей ARIMA}

Щоб підсумувати наш розгляд моделей ARIMA, обговоримо їх переваги та недоліки.

До очевидних переваг можна віднести те, що ці моделі мають дуже чітке математико-статистичне обгрунтування, що робить їх одними з найбільш науково обгрунтованих моделей з усієї безлічі моделей прогнозування тенденцій у часових рядах.

Ще однією перевагою є формалізована і найбільш докладно розроблена методика, слідуючи якій можна підібрати модель, найбільш підходящу до кожного конкретного часового ряду. Формальна процедура перевірки моделі на адекватність досить проста, а розроблені методики з автоматичного підбору найкращої ARIMA ще більше полегшують роботу.

Крім того, точкові і інтервальні прогнози випливають з самої моделі і не вимагають окремого оцінювання.

Один з явних недоліків моделей полягає у вимозі до рядів даних: для побудови адекватної моделі ARIMA потрібно не менше 40 спостережень, що на практиці не завжди можливо.

Другим серйозним недоліком є неадаптивность моделей авторегресії: при отриманні нових даних модель потрібно періодично переоцінювати.

Третій недолік полягає в тому, що побудова задовільною моделі ARIMA вимагає великих витрат ресурсів і часу.

\subsubsection{Результати дослідження методу ARIMA}

Моделі ARIMA досить гнучкі і можуть описувати широкий спектр характеристик часових рядів, що зустрічаються на практиці. Формальна процедура перевірки моделі на адекватність проста і доступна. Крім того, прогнози і інтервали передбачення слідують безпосередньо з підібраною моделі[5].

Для кожного конкретного випадку варто звертатися до своєї прогнозної моделі: будь то найпростіші моделі, моделі трендів, сезонної декомпозиції, моделі експоненціального згладжування або моделі авторегресії з ковзаючою середньою. Просто варто мати на увазі як позитивні, так і негативні сторони використовуваних моделей і спиратися на ті прогнози, щодо яких (на основі експертної думки та фундаментального аналізу галузі) можна сказати, що вони краще опишуть реальну ситуацію в майбутньому.

\subsection{Сингулярний спектральний аналіз}

Алгоритм SSA (від англійського singular spectrum analysis) $-$ метод аналізу і прогнозу часових рядів. 

Результатом застосування методу є розкладання тимчасового ряду на прості компоненти: повільні тренди, сезонні та інші періодичні або коливальні складові, а також шумові компоненти. Отримане розкладання може служити основою прогнозування як самого ряду, так і його окремих складових. SSA допускає природне узагальнення на багатовимірні тимчасові ряди, а також на випадок аналізу зображень.

Але, перш ніж перейти до самого алгоритму, необхідно розглянути сингулярний розклад SVD (від англійського singular value decomposition).

\subsubsection{Сингулярний розклад}

Сингулярний розклад є зручним методом при роботі з матрицями. Cингулярний розклад показує геометричну структуру матриці і дозволяє наочно представити наявні дані. 

SVD використовується при вирішенні найрізноманітніших завдань - від наближення методом найменших квадратів і рішення систем рівнянь до стиснення і розпізнавання зображень. Використовуються різні властивості сингулярного розкладання, наприклад, здатність показувати ранг матриці і наближати матриці даного рангу. Так як обчислення рангу матриці - завдання, яке зустрічається дуже часто, то сингулярне розкладання є досить популярним методом [7].

Нехай $X$ $-$ ненульова матриця з $L > 1$ рядками та $K > 1$ cтовпцями.

Тоді $S = XX^{T}$ $-$ симетрична матриця $L \times L$. $S$ має $L$ лінійно-незалежних власних векторів, або існують такі лінійно-незалежні вектори $U_{1}, \dots, U_{L}$, що:

\[
SU_{i} = \lambda_{i}U_{i}
\]

\noindent де $\lambda_{i}$ $-$ дійсні числа, які називають власними числами матриці $S$. 

\vspace{1.5em}

Лінійна оболонка власних векторів $U_{i}$ називається власним підпростіром [8].

Ми можемо обрати власні вектори $U_{i}$ отронормованими, тобто:

\begin{enumerate}
	\item $(U_{i}, U_{j}) = 0$, якщо $i \ne j$ (властивість ортогональності);
	\item $||U_{i}|| = 1$ (нормалізація).
\end{enumerate}.

Матриця $S$ є невід'ємно визначеною, тобто усі $\lambda_{i} \ge 0$.

Також, припустимо, що всі власні числа $\lambda_{i}$ спадають, тобто: 

\begin{equation}
\lambda_{1} \ge \lambda_{2} \ge \dots \ge \lambda_{L} \ge 0.
\end{equation}

Нехай $d$ $-$ кількість ненульових власних чисел матриці $S$. Якщо $d < L, \, \lambda_{d} > 0 \, \text{та} \, \lambda_{d+1} = 0$, то всі інші власні числа з номерами, білшими від $d$, є нульовими. Якщо $\lambda_{L} > 0$, то $d = L$ [8]. 

Оскільки значення $d$ дорівнює рангу матриці $X$, маємо: $d \le min(L, K)$.

Нехай для всіх $i$ таких, що $1 \le i \le d$ виконується формула (\ref{eq:vi}):

\begin{equation}\label{eq:vi}
V_{i} = \frac{1}{\sqrt{\lambda_{i}}}X^{T}U_{i}.
\end{equation}

Припустимо, що вектори $U_{i}$ та $V_{i}$ мають наступні властивості:

\begin{enumerate}
	\item нехай $1 \le i, j \le d$. Тоді $(V_{i}, V_{j}) = 0$ для всіх $i \ne j$ та $||V_{i}|| = 1$, якщо\newline
\hspace*{-18mm}$i > d$, тоді $X^{T}U_{i}=0_{K} \in \mathds{R}^{K}$, де $0_{K}$ $-$ нульовий вектор;
	\item $V_{i}$ є власним вектором матриці $X^{T}X$, відповідно власному числу;
	\item якщо $1 \le i \le d$, тоді $U_{i} = \sum^d_{i=1}\sqrt{\lambda_{i}}U_{i}V_{i}^{T}$;
	\item якщо $K > d$, тоді всі решта $K - d$ власних вектора матриці $X^{T}X$\newline
\hspace*{-18mm}відповідають нульовим власним числам.

\end{enumerate}

Якщо всі властивості виконуються, тоді справедлива справедлива формула:

\begin{equation}\label{eq:SVD}
X = \sum_{i=1}^{d}\sqrt{\lambda_{i}}U_{i}V_{i}^{T}.
\end{equation}

\vspace{1.5em}

Рівність (\ref{eq:SVD}) називають сингулярним розкладом (SVD) матриці $X$. Числа $\lambda_{i}$ $-$ сингулярними числами матриці $X$, а вектори $U_{i}$ та $V_{i}$ називають лівим та правим сингулярними векторами матриці $X$. Набір $(\sqrt{\lambda_{i}}, \, U_{i}, \, V_{i})$ називають $i$-ою власною трійкою матриці $X$ [8].

Єдиність сингулярного розкладу не може сприйматися буквально. Її можна сформулювати використавши наступні твердження.

Нехай $P_{1}, \dots, P_{L}$ та $Q_{1}, \dots, Q_{L}$ $-$ деякі ортонормовані системи у $\mathds{R}^{L}$ та $\mathds{R}^{K}$ відповідно. Припустимо, що існують такі константи $c_{1} \ge \dots \ge c_{L} \ge 0$, що виконується формула (\ref{eq:SVD2}).

\begin{equation}\label{eq:SVD2}
X = \sum_{i=1}^{L}c_{i}P_{i}Q_{i}^{T}.
\end{equation}

\vspace{1.5em}

Розглянемо сингулярний розклад (\ref{eq:SVD}) матриці $X$. В такому випадку:

\begin{enumerate}
	\item $c_{d} > 0$ та $c_{d+1} = \dots = c_{L} = 0$;
	\item $c_{i}^{2} = \lambda_{i}$ для $1 \le i \le d$;
	\item Для кожного $i = 1, \dots, d$ вектор $P_{i}$ є власним вектором матриці\newline
\hspace*{-18mm}$XX^{T}$, відповідним власному числу $\lambda_{i}$;
	\item $Q_{i} = X^{T}P_{i}/\sqrt{\lambda_{i}}, \, (i=1, \dots, d)$;
	\item У випадку коли всі числа $c_{i}$ різні, то (\ref{eq:SVD2}) співпадає з (\ref{eq:SVD}) з\newline 
\hspace*{-18mm}точністю до знаків $U_{i}$ та $V_{i}$.
\end{enumerate}

Нехай $I \subset \{1, \dots, d\}, \, J=\{1, \dots, d\} \textbackslash I$, $X_{I} = \sum_{i \in I}\sqrt{\lambda_{i}}U_{i}V_{i}$ та $X_{J} = X - X_{I}$. Тоді розклад $X_{J} = \sum_{i \in J}\sqrt{\lambda_{i}}U_{i}V_{i}$ є сингулярним розкладом матриці $X_{J}$.

Розглянемо матричну форму сингулярного розкладу.

Сингулярний розклад (\ref{eq:SVD}) можна переписати в матричній формі наступним чином.

Нехай $U_{d} = \left[ U_{1} : \dots : U_{d} \right]$, $V_{d} = \left[ V_{1} : \dots : V_{d}\right]$ та $\Lambda_{d}$ $-$ диагональна $d \times d$ матриця з власними числами $\lambda_{i}$ в якості $i$-х диагональних елементів.

Тоді формулу (\ref{eq:SVD}) можна записати наступним чином:

\begin{equation}\label{eq:SVDMatrix}
X = U_{d}\Lambda_{d}^{1/2}V_{d}^{T},
\end{equation}

\noindent що є стандартною матричною формою сингулярного розкладу.

\vspace{1.5em}

Рівність (\ref{eq:SVDMatrix}) можна переписати у формі, відомій як квазі-діагональна форма матриці $X$. Як відомо, при відповідному виборі ортонормованого базису у $\mathds{R}^{L}$ будь-яка симетричка $L \times L$ матриця має совє діагональну форму. З (\ref{eq:SVDMatrix}) випливає, що можна обрати відповідні базиси у $\mathds{R}^{L}$ та $\mathds{R}^{K}$ так, шоб отримати аналогічну форму для прямокутної матриці $X$.

Нехай $U = [U_{1}: \dots : U_{L}]$, $V = [V_{1}: \dots : V_{K}]$. Матриці $U_{L \times L}$ та $V_{K \times K}$ $-$ унітарні матриці. Для матриці $U$ це означає, що для будь-яких векторів $X,Y \in \mathds{R}^{L}$ рівність $(UX, UY) = (X, Y)$ є справедливою і, відповідно, матриця $U$, розглядаєма як лінійне відображення $\mathds{R}^{L} \mapsto \mathds{R}^{L}$, зберігає векторні норми та кути між векторами. Іншою характеристикою властивості унітарності є тотожність $U^{-1}=U^{T}$.

Позначимо $\Lambda$ матрицю тієї ж розмірності як і матриця $X$ з діагональними елементами $\lambda_{ii} = \lambda_{i}$ за умови $1 \le i \le d$ та всіми іншими елементами, які дорівнюють нулю. Тоді (\ref{eq:SVDMatrix}) можна переписати у вигляді формули:

\begin{equation}\label{eq:kvazidiagX}
X = U\Lambda^{1/2}V^{T} \, \text{або} \, \Lambda^{1/2}=U^{T}XV
\end{equation}

\vspace{1.5em}

Рівність (\ref{eq:kvazidiagX}) $-$ це квазі-діагональна форма матриці $X$. При відповідних базисах $U_{1}, \dots, U_{L}$ у $\mathds{R}^{L}$ та $V_{1}, \dots, V_{K}$ у $\mathds{R}^{K}$, будь-яка прямокутна матриця розмірності $L \times K$ має квазі-діагональну форму $\Lambda^{1/2}$.

Сингулярний розклад є потужним обчислювальним інструментом. Сучасні алгоритми отримання такої декомпозиції загальних матриць мали глибокий вплив на численні застосування в науково-технічних дисциплінах. SVD зазвичай використовується у вирішенні незміщених лінійних задач найменших квадратів, оцінки матричного рангу та канонічного кореляційного аналізу. В обчислювальній науці вона зазвичай застосовується в таких областях, як пошук інформації, сейсмічна рефлекторна томографія і обробка сигналів у реальному часі [9].

\subsubsection{Алгоритм сингулярного спектрального аналізу}

Нехай $F = (f_{0}, \dots, f_{N-1})$ $-$ часовий ряд що складається з дійсних значень довжини $N$, де $N > 2$. Передбачається що ряд $F$ має хоча б одне нульове значення.

Алгоритм SSA в своїй базовоей версії має чотири формальних кроки: 

\begin{enumerate}
	\item вкладення; 
	\item сингулярний розклад;
	\item угруповання;
	\item діагональне усереднення. 
\end{enumerate}	

На етапі вкладення припускається, що $L$ - ціле число, таке що $1 < L < N$, яке назвемо довжиною вікна. Процедура вкладення перетворює вихідний часовий ряд в $K = N - L + 1$ векторів вкладення $x_{i}$ за таким правилом:

\begin{equation}
x_{i} = (f_{i-1}, \dots, f_{i+L-2})^{T}, \, 1 \le i \le K.
\end{equation}

\vspace{1.5em}

Отримані вектори $x_{i}$ мають розмірність $L$, тому якщо в подальшому знадобиться підкреслити їх розмірність будемо називати їх векторами $L$-вкладення.

З векторів $L$-вкладення $x_{i}$ ми отримуємо $L$-траєкторну матрицю $X$ ряду $F$:

\begin{equation}
X = [x_{1} : x_{2} : \dots : x_{K-1} : x_{K}].
\end{equation}

\vspace{1.5em}

Тобто матриця $X$ відносто елементів ряду $F$ буде мати наступний вигляд:

\begin{equation}\label{eq:traekt}
X = \left(\begin{array}{ccccc} 
f_{0} & f_{1} & f_{2} & \dots & f_{K-1} \\
f_{1} & f_{2} & f_{3} & \dots & f_{K} \\
f_{2} & f_{3} & f_{4} & \dots & f_{K+1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
f_{L-1} & f_{L} & f_{L+1} & \dots & f_{N-1} \\
\end{array}\right).
\end{equation}

\vspace{1.5em}

Етап сингулярного розкладу передбачає, що $S = XX^{T}$ та $\lambda_{1}, \dots, \lambda_{L}$ $-$ власні числа матриці $S$, такі що $\lambda_{1} \ge \lambda_{2} \ge \dots \ge \lambda_{L} \ge 0$. Нехай $u_{1}, \dots, u_{L}$ $-$ власні вектори матриці $S$ з індексами відповідними власним числам матриці $S$.

Нехай $d = \underset{\lambda_{i} \ge 0}{max}(i)$, тоді $v_{i}$ будемо обчислювати так, як запропоновано у формулі (\ref{eq:vissa}).

\begin{equation}\label{eq:vissa}
v_{i} = \frac{X^{T}u_{i}}{\sqrt{\lambda_{i}}}, \, \text{де} \, i=1 \dots d.
\end{equation}

\vspace{1.5em}

Після цього в такий спосіб може бути отримано сингулярний розклад $L$-траєкторної матриці $X$ ряду $F$. Його наведено у формулі \ref{eq:SVDmain}.

\begin{equation}\label{eq:SVDmain}
X = X_{1} + X_{2} + \dots + X_{d}, \, \text{за умови} \, X_{i} = u_{i} \cdot v_{i}^{T} \cdot \sqrt{\lambda_{i}}.
\end{equation}

\vspace{1.5em}

Матриці $X_{i}$ будемо називати елементарними матрицями сингулярного розкладу (\ref{eq:SVDmain}), набір $(\sqrt{\lambda_{i}}, u_{i}, v_{i})$ $-$ $i$-ою власної трійкою сингулярного розкладу (\ref{eq:SVDmain}), де $\sqrt{\lambda_{i}}$ $-$ сингулярні числа, а $u_{i}$ та $v_{i}$ $-$ ліві і праві сингулярні вектори.

Варто зазначити що так як рядки і стовпці вихідної траєкторної матриці є відрізками вихідного ряду. Тому лівий і правий сингулярні вектора також мають часову структуру і можуть розглядатися як часові ряди [8].

Не кожен сингулярний поділ коректно розділяє траєкторну матрицю на складові. Коректність результату поділу визначається на етапі угруповання і сильно залежить від довжини вікна.

На жаль, немає жодних правил для вибору правильної довжини вікна. Є тільки ряд безіменних теорем на основі яких складено ряд рекомедацій, який буде освітлений далі:

\begin{enumerate}
	\item не має сенс брати довжину вікна більш ніж половина ряду;
	\item чим більше довжина вікна, тим більш детальним виходить\newline
\hspace*{-18mm}розкладання вихідного ряду;
	\item мінімальна довжина вікна може призвести до змішування\newline 
\hspace*{-18mm}інтерпретованих компонент ряду;
	\item результати слабкої роздільності стійкі до малих змін довжини\newline \hspace*{-18mm}вікна;
	\item для рядів зі складною структурою занадто велика довжина вікна\newline \hspace*{-18mm}може привести до небажаного розкладання цікавлячих нас компонент;
	\item мале зміна довжини вікна може зменшити змішування.
\end{enumerate}



Етап угруповання передбачає, що на основі сингулярного розкладу (\ref{eq:SVDmain}) ми отримали множину з $d$ елементарних матриць. Процедура угруповання ділить цю множину на $m$ непересічних підмножин $I_{1}, I_{2}, \dots, I_{m}$ та називається угрупованням власних трійок. Сума елементів кожного з цих підмножин дасть нам набір траєкторних матриць, які відповідають за різні складові тренда, періодика і шуму.

Оскільки не завжди сингулярний розклад дає нам чистий розклад ряду на компоненти процедура угруповання здійсненна не завжди, що обумовлює неавтоматізацію всього методу.

Крім того, як і у випадку з довжиною вікна, немає конкретних правил для виконання угруповання, лише рекомендації, які не завжди застосовні. 

Розглянемо ці рекомендації:

\begin{enumerate}
	\item сингулярне значення описує ступінь вкладу сингулярной трійки в\newline 
\hspace*{-18mm}розкладання, чим більше значення, тим більший внесок;
	\item компонента відновлена з однієї власної трійки матиме таку ж\newline \hspace*{-18mm}форму як і відповідні сингулярні вектори;
	\item факторний вектор більше власного схожий на відновлювану\newline
\hspace*{-18mm}компоненту ряду при $L \ll K$;
	\item якщо $N, L, K$ доволі великі, то кожна відмінна від пилкоподібної\newline
\hspace*{-18mm}гармонійна компонента породжує пару власних трійок з близькими\newline \hspace*{-18mm}власними числами;
	\item при виділенні пар гармонік з близькими амплітудами відповідні\newline
\hspace*{-18mm}факторні (і власні) вектори власних трійок з близькими значеннями\newline
\hspace*{-18mm}утворюють точки лежать на колі.
\end{enumerate}


Останнім епатом є діагональне усереднення.
Даний етап відновлює з сум згрупованих на попередньому етапі елементарних матриць власне складові ряду.

Так як вихідна матриця $X$ є $L$-траєкторної, то і всі елементарні матриці так само є $L$-траєкторні. Однак на практиці таке майже ніколи не проявляється і зберігається тільки розмірність матриць.

Але інформація в цих матрицях все ще відповідає характеристикам згрупованих складових вихідного ряду. Тому має місце етап діагонального усереднення.

Нехай $Y$ $-$ матриця розмірності $L \times K$ з елементами $y_{i,j}$, де $1 \le i \le L$, $1 \le j \le K$. Нехай $L^{*} = min(L, K), \, K^{*} = max(L,K), \, N = L + K - 1$. Нехай $y_{ij}^{*} = y_{ij}$ якщо $L < K$ та $y_{ij}^{*} = y_{ji}$ в іншому випадку.

Тоді діагональне усереднення, яке переводить матрицю $Y$ розмірності $L \times K$ у ряд $g_{0}, g_{1}, \dots, g_{N-1}$ виражається наступним чином:

\begin{equation}
g_{k} = \begin{cases}
\frac{1}{k+1}\sum_{m=1}^{k+1}y_{m,k-m+2}^{*}, &\text{коли } 0 \le k < L^{*} - 1; \\
\frac{1}{L^{*}}\sum_{m=1}^{L^{*}}y_{m,k-m+2}^{*}, &\text{коли } L^{*} - 1 \le k < K^{*}; \\
\frac{1}{N-k}\sum_{m=k-K^{*}+1}^{N-K^{*}+1}y_{m,k-m+2}^{*}, &\text{коли } K^{*} \le k < N.
\end{cases}
\end{equation}

\vspace{1.5em}


Спочатку передбачається, що ряд $F$ є сумою $m$ рядів. Але отримати ці ряди як результат угруповання елементарних сингулярних матриць не є тривіальним завданням. Щоб її вирішити необхідно провести діагональне усереднення над елементарними матрицями і перевірити отримані ряди на роздільність. Після проведення даного дослід вання можна зробити висновок про те, чи правильно обрана довжина вікна і як групувати елементарні матриці в $m$ компонент ряду $F$.


Нехай $F_{N}^{(1)}$ та $F_{N}^{(2)}$ $-$ часові ряди довжини $N$, та $F_{N} = F_{N}^{(1)} + F_{N}^{(2)}$. Нехай за довжини вікна $L$ кожен з рядів $F_{N}$, $F_{N}^{(1)}$ та $F_{N}^{(2)}$ породжує $L$-траєкторні матриці $X$, $X^{(1)}$ та $X^{(2)}$ [8].

Нехай $\Lambda^{(L,1)}$, $\Lambda^{(L,2)}$ $-$ лінійні простори, породжені стовпцями траєкторних матриць $X^{(1)}$ та $X^{(2)}$ відповідно. Аналогічно $\Lambda^{(K,1)}$, $\Lambda^{(K,2)}$ $-$ лінійні простори, породжені стовпцями траєкторних матриць $(X^{(1)})^{T}$ та $(X^{(2)})^{T}$ відповідно.

Ряди $F_{N}^{(1)}$ та $F_{N}^{(2)}$ називають слабко $L$-роздільними, якщо:

\begin{equation}
	\left\{
	\begin{aligned}
	\Lambda^{(L,1)} \bot \Lambda^{(L,2)}; \\
	\Lambda^{(K,1)} \bot \Lambda^{(K,2)}.
	\end{aligned}
	\right.
\end{equation}

\vspace{1.5em}

Якщо ряди $F_{N}^{(1)}$ і $F_{N}^{(2)}$ слабко $L$-роздільні і множина власних чисел розкладання траєкторної матриці одного ряду не перетинається з множиною власних чисел сингулярного розкладання іншого ряду, то ряди $F_{N}^{(1)}$ і $F_{N}^{(2)}$ називаються сильно $L$-роздільними.

Розглядаючи роздільність різних тестових рядів, а зокрема константний ряд, експонента, синус / косинус, косинус-екпонента і поліноміальний ряд, в табл. \ref{tab:exactsep} робиться висновок про принаймні теоретичної слабкою разделимости рядів цих типів:

\begin{table}[h]
	\captionstyle{ \raggedright}
	\caption{Точна роздільність}\label{tab:exactsep}
	%\centering
	\begin{tabular}{| m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} |}
	\hline
	 & const & cos & exp & exp cos & ak+b \\
	\hlinewd{2pt}
	const & - & + & - & - & - \\
	\hline
	cos & + & + & - & - & - \\
	\hline
	exp & - & - & - & + & - \\
	\hline
	exp cos & - & - & + & + & - \\
	\hline
	ak + b & - & - & - & - & - \\
	\hline
	\end{tabular}
\end{table}

У таблиці знаком «+» позначені пари, для яких існують параметри функцій і параметри методу $L$ і $K = N - L + 1$, при яких ряди слабко $L$-роздільні. Таблиця \ref{tab:exactsep} показує що умови разделимости є досить жорсткими [8].

Через жорсткості рамок умови слабкої та більш сильної роздільності майже не застосовні в реальних задачах. Тому вводиться поняття наближеною разделимости, яке використовує ідею ортогональности лінійних просторів породжених стовпцями траєкторних матриць.

Для будь-якого ряду $F_{N} = (f_{0}, f_{1}, \dots, f_{N-1})$ приймемо:

\begin{equation}
F_{i,j} = (f_{i-1}, \dots, f_{j-1}), \text{ де } 1 \le i \le j < N.
\end{equation}


\vspace{1.5em}

Тоді визначимо коефіцієнт кореляції рядів. Він обчислюється так, як показано у формулі (\ref{eq:koefkor}).

\begin{equation}\label{eq:koefkor}
\rho_{i,j}^{(M)} = \frac{(F_{i,i+M-1}^{(1)}, F_{j,j+M-1}^{(2)})}{||F_{i,i+M-1}^{(1)}|| \cdot ||F_{j,j+M-1}^{(2)}||}.
\end{equation}


\vspace{1.5em}

Часові ряди $F_{N}^{(1)}$ та $F_{N}^{(2)}$ називають слабко $\varepsilon$-роздільними, якщо при довжині вікна $L$ та при малому $\varepsilon$ виконується нерівність (\ref{eq:epssep}).

\begin{equation}\label{eq:epssep}
\rho^{(L,K)} = max(\underset{1 \le i,j \le K}{max}|\rho_{i,j}^{(L)}|, \underset{1 \le i,j \le L}{max}|\rho_{i,j}^{(K)}|) < \varepsilon.
\end{equation}

Обираючи послідовність $1 < K = L(N) <  N$ з нерівності (\ref{eq:epssep}) можна отримати максимальні коефіцієнти кореляції $\rho_{N} = \rho^{(L(N),K(N))}$.

Якщо $\rho^{(L(N),K(N))} \longrightarrow 0$ при деякій послідовності $L=L(N)$ та $N \longrightarrow \infty$, то ряди $F_{N}^{(1)}$ та $F_{N}^{(2)}$ називають асимптотично $L(N)$-роздільними. Якщо ряди $F_{N}^{(1)}$ та $F_{N}^{(2)}$ асимптотично $L(N)$-роздільні при будь-якому виборі послідовності $L(N)$, такої що $L(N) \longrightarrow \infty$ та $K(N) \longrightarrow \infty$ їх називають асимптотично роздільними.

Аналогічно точної роздільності в табл. \ref{tab:assep} приведений результат дослідження асимптотичної роздільності.

\begin{table}[h]
	\captionstyle{ \raggedright}
	\caption{Асимптотична роздільність}\label{tab:assep}
	%\centering
	\begin{tabular}{| m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} | m{0.1\textwidth} |}
	\hline
	 & const & cos & exp & exp cos & ak+b \\
	\hlinewd{2pt}
	const & - & + & + & + & - \\
	\hline
	cos & + & + & + & + & + \\
	\hline
	exp & + & + & + & + & + \\
	\hline
	exp cos & + & + & + & + & + \\
	\hline
	ak + b & - & + & + & + & - \\
	\hline
	\end{tabular}
\end{table}

Таблиця \ref{tab:assep} показує що асімтотічна роздільність застосовна для набагато більш широкого набору класів рядів, ніж точна роздільність.

До сих пір розглядалися умови тільки слабкою разделимости рядів. Однак, навіть при наявності слабкої роздільності, але відсутності сильної робить практично неможливою поділ цих рядів. Через співпадаючі власні числа варіант сингулярного розкладу, який має місце на практиці не буде розділяти ряди.


\subsubsection{Результати дослідження методу SSA}

Ціллю метода є розкладання часового ряду на інтерпретовані адитивні складові. При цьому метод не потребує стаціонарності ряду, знань моделі тренду, а також відомостей о наявності у ряді періодичних складових та іхніх періодах.

За таких слабких припущень метод SSA може вирішувати різноманітні задачі, такі як, наприклад, виділення тренду, виявлення періодик, згладжування ряду, побудова полного розкладу ряду у суму тренду, періодик та шуму.

З іншого боку, платою за такий широкий спектр можливостей при досить слабких припущеннях є, по-перше, істотне неавтоматичне групування компонент сингулярного розкладу траєкторної матриці ряду задля отримання складових вихідного ряду, 

По-друге, відсутність моделі не дає можливості перевірити гіпотези про наявність тієї чи іншої складової. Для перевірки подібних гіпотез необхідна побудова моделі, яка, в свою чергу, може бути проведена базуючись на інформації, яку ми отримали за допомоги методу SSA.

Розглядаємий непараметричний метод дозволяю отримати результати не гірше, ніж більшість параметричних методів, коли аналізується ряд з відомою моделлю.

\newpage

\subsection{Висновки за розділом}
В результаті огляду літературних джерел були обрані наступні методи:

\begin{enumerate}
	\item модель авторегресії$-$проінтегрованого ковзного середнього;
	\item алгоритм сингулярного спектрального аналізу.
\end{enumerate}

Вони найбільше підходять для вирішення задач поставлених у дипломній роботі.
