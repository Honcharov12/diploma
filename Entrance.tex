%\anonsection{Вступ}
\anonsection{ВСТУП}
\addcontentsline{toc}{section}{Вступ}

Ми живемо в часи, коли інформаційні технології почали займати визначальну роль у житті людства. Зараз важко уявити життя без телефонів, планшетів, комп'ютерів, розумних годинників та інших девайсів, за допомогою яких можна покращити якість сна, автоматизувати процеси на виробництві, дізнатись думку населення з певного питання та безліч інших сфер діяльності в яких застосовуються інформаційні технології. Завдяки зростанню обчислювальних потужностей та об'ємів зберігаємих даних став можливим новий стрибок у розвитку машинного навчання, завдяки якому, в свою чергу, технології почали ще краще поліпшувати наше життя. В останні роки дуже стрімко розвиваються нейронні мережі (neural networks) та глибинні нейронні мережі (deep neural networks, DNN). Є багато напрямків використання нейронних мереж, наприклад самонавчання, дослідження певних алгоритмів якого є темою даної дипломної роботи.

Термін самонавчання (self-supervised learning, SSL) використовувався у різних контекстах та сферах, таких як навчання уявлень (representation learning), нейронні мережі, робототехніка, обробка природних мов (natural language processing, NLP) та навчання з посиленням (reinforcement learning). У всіх випадках основна ідея полягає в тому, щоб автоматично генерувати якийсь наглядовий сигнал для вирішення якогось завдання (як правило, вивчати подання даних або автоматично розмічати набір даних).

В типовій задачі SSL є величезна нерозмічена вибірка, необхідно сформувати для кожного об'єкта псевдо-мітку (pseudo label) і вирішити отриману SSL-завдання, але нас цікавить не стільки якість рішення придуманого нами завдання (її називають pretext task), скільки представлення (representation) об'єктів, яке буде вивчено в процесі її рішення. Це представлення можна в подальшому використовувати вже при вирішенні будь-якої задачі з мітками (SSL), яку називають наступною задачею (downstream task). Одна з головних причин самонавчання $-$ невеликий обсяг розмічених даних. На відміну від навчання з частково розміченими даними в самонавчанні використовуються абсолютно довільні нерозмічену дані, що не мають відношення до розв'язуваної задачі.

Сучасна обробка тексту (NLP) приблизно на 80\% складається з самонавчання. Наприклад, за допомогою самонавчання можна знайти майже всі представлення слів (а також текстів). Наприклад, в класичному алгоритмі word2vec беруть нерозмічений корпус текстів, потім самі придумують завдання з мітками (по сусіднім словами передбачити центральне слово або, навпаки, по центральному сусідні з ним), навчають на цьому завданні просту нейронну мережу, в результаті виходять векторні уявлення слів, які вже використовуються в інших, ніяк не пов'язаних з попередньою, задачах. Часто такий підхід називають також трансферним навчанням (transfer learning). Вцілому, трансферне навчання більш широке поняття - коли модель, навчену для вирішення однієї задачі, використовують при вирішенні іншої. У самонавчанні важливо, щоб розмітка в попередньої задачі (псевдо-мітки) виходила автоматично. Наприклад, представлення, отримані методом Cove, в якому використовується кодер для завдання машинного перекладу, будуть трансферними, але не отриманими за допомогою самонавчання.

Останнім часом, набирає популярність підрозділ самонавчання $-$ порівняльне навчання (constrastive learning).

Метою даної дипломної роботи є дослідження роботи алгоритмів порівняльного навчання Deep InfoMax та Momentum Contrast.

