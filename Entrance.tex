%\anonsection{Вступ}

\begin{center}
\textbf{ВСТУП}
\end{center}
\addcontentsline{toc}{section}{Вступ}

%\hspace*{26pt}
Ми живемо в часи, коли інформаційні технології почали займати визначальну роль у житті людства. Зараз важко уявити життя без телефонів, планшетів, комп'ютерів, розумних годинників та інших девайсів, за допомогою яких можна покращити якість сна, автоматизувати процеси на виробництві, дізнатись думку населення з певного питання та безліч інших сфер діяльності в яких застосовуються інформаційні технології. Завдяки зростанню обчислювальних потужностей та об'ємів зберігаємих даних став можливим новий стрибок у розвитку машинного навчання, завдяки якому, в свою чергу, технології почали ще краще поліпшувати наше життя. В останні роки дуже стрімко розвиваються нейронні мережі (neural networks) та глибинні нейронні мережі (deep neural networks, DNN). Існує багато різноманітних методологій та підрозділів нейронних мереж, одним з яких є самонавчання.

Термін самонавчання (self-supervised learning, SSL) використовувався у різних контекстах та сферах, таких як навчання уявлень (representation learning), нейронні мережі, робототехніка, обробка природних мов (natural language processing, NLP) та навчання з посиленням (reinforcement learning). У всіх випадках основна ідея полягає в тому, щоб автоматично генерувати якийсь контрольний сигнал для вирішення якогось завдання (як правило, вивчати подання даних або автоматично розмічати набір даних) [1].

В типовiй SSL задачi ми на входi маємо величезну нерозмiчену вибiрку, SSL-завдання полягае у тому, що необхiдно розробити алгоритм, що для кожного об’єкта сформує псевдо-мiтку (pseudo label), але нас цікавить не стільки якість рішення придуманого нами завдання (її називають pretext task), скільки представлення (representation) об'єктів, яке буде вивчено в процесі її рішення. Це представлення можна в подальшому використовувати вже при вирішенні будь-якої задачі з мітками, яку називають наступною задачею (downstream task). Одна з головних причин самонавчання $-$ невеликий обсяг розмічених даних. На відміну від навчання з частково розміченими даними в самонавчанні використовуються абсолютно довільні нерозмічену дані, що не мають відношення до розв'язуваної задачі [2].

Сучасна обробка тексту (NLP) приблизно на 80\% складається з самонавчання. Наприклад, за допомогою самонавчання можна знайти майже всі представлення слів (а також текстів). Наприклад, в класичному алгоритмі word2vec беруть нерозмічений корпус текстів, потім самі придумують завдання з мітками (по сусіднім словами передбачити центральне слово або, навпаки, по центральному передбачити сусідні із ним), навчають на цьому завданні просту нейронну мережу, в результаті виходять векторні уявлення слів, які вже використовуються в інших, ніяк не пов'язаних з попередньою, задачах. Часто такий підхід називають також трансферним навчанням (transfer learning, TL). Вцілому, трансферне навчання більш широке поняття $-$ коли модель, навчену для вирішення однієї задачі, використовують для вирішення іншої. У самонавчанні важливо, щоб розмітка в попередньої задачі (псевдо-мітки) отримувалась автоматично. 
SSL і TL мають переваги та недоліки. Важко судити, чи один кращий за інший. Існуючі емпіричні результати показують, що в певних завданнях SSL перевершує рівень TL; в інших завданнях TL працює краще, ніж SSL [3].


%Наприклад, представлення, отримані методом Cove, в якому використовується кодер для завдання машинного перекладу, будуть трансферними, але не отриманими за допомогою самонавчання.

Останнім часом, набирає популярність підрозділ самонавчання $-$ порівняльне навчання (constrastive learning).

Метою даної дипломної роботи є дослідження роботи алгоритмів порівняльного навчання Deep InfoMax та Momentum Contrast.

Були поставлені наступні задачі:

\begin{enumerate}
	\item вибір даних для аналізу роботи алгоритмів;
	\item реалізація методів Deep InfoMax та Momentum Contrast;
	\item дослідження роботи вищеназваних методів на обраних даних;
	\item порівняння роботи алгоритмів.
\end{enumerate}

